apiVersion: v1
kind: ConfigMap
metadata:
  name: flux-sample-flux-config
  namespace: default
data:
  hostfile: "# Flux needs to know the path to the IMP executable\n[exec]\nimp = \"/usr/libexec/flux/flux-imp\"\n\n[access]\nallow-guest-user
    = true\nallow-root-owner = true\n\n# Point to resource definition generated with
    flux-R(1).\n[resource]\npath = \"/etc/flux/system/R\"\n\n[bootstrap]\ncurve_cert
    = \"/etc/curve/curve.cert\"\ndefault_port = 8050\ndefault_bind = \"tcp://eth0:%p\"\ndefault_connect
    = \"tcp://%h.flux-service.default.svc.cluster.local:%p\"\nhosts = [\n\t{ host=\"flux-sample-m-0-[0-3]\"},\n]\n[archive]\ndbpath
    = \"/var/lib/flux/job-archive.sqlite\"\nperiod = \"1m\"\nbusytimeout = \"50s\"\n\n#
    Configure the flux-sched (fluxion) scheduler policies\n# The 'lonodex' match policy
    selects node-exclusive scheduling, and can be\n# commented out if jobs may share
    nodes.\n[sched-fluxion-qmanager]\nqueue-policy = \"fcfs\""
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: flux-sample-curve-mount
  namespace: default
data:
  curve.cert: "#   ****  Generated on 2023-04-26 22:54:42 by CZMQ  ****\n#   ZeroMQ
    CURVE **Secret** Certificate\n#   DO NOT PROVIDE THIS FILE TO OTHER USERS nor
    change its permissions.\n    \nmetadata\n    name = \"flux-cert-generator\"\n
    \   keygen.hostname = \"flux-sample-m-0-0\"\ncurve\n    public-key = \"xaRj#UPiQrn:Y4LyU8QS1UWBGci$vIO/sz+gvHe!\"\n
    \   secret-key = \"C^NtAB7Uil%3LO4E)Pxuxmb}yoy87cKP(L$W57.R\"\n"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: flux-sample-entrypoint
  namespace: default
data:
  wait-0: "#!/bin/sh\n\n# This waiting script is run continuously and only updates\n#
    hosts and runs the job (breaking from the while) when\n# update_hosts.sh has been
    populated. This means the pod usually\n# needs to be updated with the config map
    that has ips!\n\n# If any initCommand logic is defined\n > /dev/null\n\n# If we
    are not in strict, don't set strict mode\nset -eEu -o pipefail\n\n# Set the flux
    user from the getgo\nfluxuser=flux\nfluxuid=1000\nfluxroot=/usr\nif [ \"${fluxroot}\"
    == \"\" ]; then\n  fluxroot=\"/usr\"\nfi\n\n\n\n# Ensure pythonpath is set to
    something\nif [ -z ${PYTHONPATH+x} ]; then\n  PYTHONPATH=\"\"\nfi\nif [ -z ${LD_LIBRARY_PATH+x}
    ]; then\n  LD_LIBRARY_PATH=\"\"\nfi\n\n# Set the flux root\n\nexport fluxroot\n\n#
    commands to be run as root\nasSudo=\"sudo -E PYTHONPATH=$PYTHONPATH -E PATH=$PATH
    -E LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\"\n\n# Always run flux commands (and the
    broker) as flux user, unless requested otherwise (e.g., for storage)\n\n# and
    ensure the home is targeted to be there too.\nasFlux=\"sudo -u ${fluxuser} -E
    PYTHONPATH=$PYTHONPATH -E PATH=$PATH -E LD_LIBRARY_PATH=${LD_LIBRARY_PATH} -E
    HOME=/home/${fluxuser}\"\n\n\n# And pre command logic that isn't passed to the
    certificate generator\n > /dev/null 2>&1\n\n# We currently require sudo and an
    ubuntu base\nwhich sudo > /dev/null 2>&1 || (echo \"sudo is required to be installed\"
    && exit 1);\nwhich flux > /dev/null 2>&1 || (echo \"flux is required to be installed\"
    && exit 1);\n\n# Add fluxuser to sudoers, only if not running as root\n\necho
    \"${fluxuser} ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers\n\n# Add a flux user (required)
    that should exist before pre-command\nsudo adduser --disabled-password --uid ${fluxuid}
    --gecos \"\" ${fluxuser} > /dev/null 2>&1 || true\n\n# Show user permissions /
    ids\n\n\n\n\n\n\n\n# We use the actual time command and not the wrapper, otherwise
    we get there is no argument -f\n\n\n# If the user wants to save/load archives,
    we set the state directory to that\n# The statedir similarly should exist and
    have plenty of available space.\nexport STATE_DIR=/var/lib/flux\nexport FLUX_OUTPUT_DIR=/tmp/fluxout\nmkdir
    -p ${STATE_DIR} ${FLUX_OUTPUT_DIR}\n\n# Broker Options: important!\n# The local-uri
    setting places the unix domain socket in rundir\n#   if FLUX_URI is not set, tools
    know where to connect.\n# TODO newer flux will not have quorum=ranks, but rather
    -Sbroker.quorum=2 (size)\nbrokerOptions=\"-Scron.directory=/etc/flux/system/cron.d
    \\\n  -Stbon.fanout=256 \\\n  -Srundir=/run/flux  \\\n  -Sstatedir=${STATE_DIR}
    \\\n  -Slocal-uri=local:///run/flux/local \\\n \\\n \\\n \\\n -Slog-stderr-level=0
    \ \\\n  -Slog-stderr-mode=local\"\n\n# if we are given an archive to use, load
    first, not required to exist\n# Note that we ask the user to dump in interactive
    mode - I am not\n# sure that doing it with a hook ensures the dump will be successful.\n\n\n#
    quorum settings influence how the instance treats missing ranks\n#   by default
    all ranks must be online before work is run, but\n#   we want it to be OK to run
    when a few are down\n# These are currently removed because we want the main rank
    to\n# wait for all the others, and then they clean up nicely\n#  -Sbroker.quorum=0
    \\\n#  -Sbroker.quorum-timeout=none \\\n\n# Run diagnostics instead of a command\nrun_diagnostics()
    {\n    printf \"\\n\U0001F438 ${asFlux} flux start -o --config /etc/flux/config
    ${brokerOptions} flux overlay status\\n\"\n    ${asFlux} flux start -o --config
    /etc/flux/config ${brokerOptions} flux overlay status\n    printf \"\\n\U0001F438
    ${asFlux} flux start -o --config /etc/flux/config ${brokerOptions} flux lsattr
    -v\\n\"\n    ${asFlux} flux start -o --config /etc/flux/config ${brokerOptions}
    flux lsattr -v\n    printf \"\\n\U0001F438 ${asFlux} flux start -o --config /etc/flux/config
    ${brokerOptions} flux dmesg\\n\"\n    ${asFlux} flux start -o --config /etc/flux/config
    ${brokerOptions} flux dmesg\n    printf \"\\n\U0001F4A4 sleep infinity\\n\"\n
    \   sleep infinity\n}\n\n# Cron directory\nmkdir -p /etc/flux/system/cron.d\n\n#
    Main host <name>-0 and the fully qualified domain name\nmainHost=\"flux-sample-m-0-0\"\n\n#
    The working directory should be set by the CRD or the container\nworkdir=${PWD}\n\n#
    And if we are using fusefs / object storage, ensure we can see contents\nmkdir
    -p ${workdir}\n\n# We always do a listing in case it's needed to \"expose\" object
    storage\n# Often this isn't enough and a list of the paths needed should be\n#
    added to containers[].commands.pre\n\nls -R ${workdir} > /dev/null 2>&1\n\n\n#
    These actions need to happen on all hosts\n# Configure resources\nmkdir -p /etc/flux/system\n\n#
    --cores=IDS Assign cores with IDS to each rank in R, so we  assign 0-(N-1) to
    each host\n\nflux R encode --hosts=flux-sample-m-0-[0-3] --local > /etc/flux/system/R\n\n\n#
    Do we want to run diagnostics instead of regular entrypoint?\ndiagnostics=\"false\"\n\n\n#
    Flux option flags\noption_flags=\"\"\nif [ \"${option_flags}\" != \"\" ]; then\n
    \   # Make sure we don't get rid of any already defined flags\n    existing_flags=\"${FLUX_OPTION_FLAGS:-}\"\n\n
    \   # provide them first so they are replaced by new ones here\n    if [ \"${existing_flags}\"
    != \"\" ]; then\n        export FLUX_OPTION_FLAGS=\"${existing_flags} ${option_flags}\"\n
    \   else\n        export FLUX_OPTION_FLAGS=\"${option_flags}\"\n    fi\n\nfi\n\nmkdir
    -p /etc/flux/imp/conf.d/\n\n\ncat <<EOT >> /etc/flux/imp/conf.d/imp.toml\n[exec]\nallowed-users
    = [ \"${fluxuser}\", \"root\" ]\nallowed-shells = [ \"${fluxroot}/libexec/flux/flux-shell\"
    ]\nEOT\n\n\n\n\n# If we are communicating via the flux uri this service needs
    to be started\nchmod u+s ${fluxroot}/libexec/flux/flux-imp\nchmod 4755 ${fluxroot}/libexec/flux/flux-imp\nchmod
    0644 /etc/flux/imp/conf.d/imp.toml\nsudo service munge start > /dev/null 2>&1\n\n#
    The rundir needs to be created first, and owned by user flux\n# Along with the
    state directory and curve certificate\nmkdir -p /run/flux /etc/curve\n\n# Show
    generated curve certificate - the munge.key should already be equivalent (and
    exist)\ncp /mnt/curve/curve.cert /etc/curve/curve.cert\n\n# Remove group and other
    read\nchmod o-r /etc/curve/curve.cert\nchmod g-r /etc/curve/curve.cert\n\n# Either
    the flux user owns the instance, or root\n\n\n# We must get the correct flux user
    id - this user needs to own\n# the run directory and these others\nfluxuid=$(id
    -u ${fluxuser})\n\nchown -R ${fluxuid} /run/flux ${STATE_DIR} /etc/curve/curve.cert
    ${workdir} ${FLUX_OUTPUT_DIR}\n\n# Make directory world read/writable\nchmod -R
    0777 ${workdir}\n\n\n\nfunction run_flux_restful() {\n\n    # Start restful API
    server\n    branch=main\n    startServer=\"uvicorn app.main:app --host=0.0.0.0
    --port=5000\"\n    printf \"Cloning flux-framework/flux-restful-api branch ${branch}\\n\"\n
    \   git clone -b ${branch} --depth 1 https://github.com/flux-framework/flux-restful-api
    /flux-restful-api > /dev/null 2>&1\n    cd /flux-restful-api\n            \n    #
    Export the main flux user and token \"superuser\"\n    export FLUX_USER=flux\n
    \   export FLUX_TOKEN=690202f2-6c8f-4d76-91f6-c79ecf7a97ef\n    printf \"\U0001F512️
    Credentials, my friend!\\n    FLUX_USER: ${FLUX_USER}\\n    FLUX_TOKEN: ${FLUX_TOKEN}\\n\\n\"\n\n
    \   # Install python requirements, with preference for python3\n    python3 -m
    pip install -r requirements.txt > /dev/null 2>&1 || python -m pip install -r requirements.txt
    > /dev/null 2>&1\n\n    # Prepare databases!\n    alembic revision --autogenerate
    -m \"Create intital tables\"\n    alembic upgrade head\n    python3 app/db/init_db.py
    init || python app/db/init_db.py init\n\n    \n\n    # Shared envars across user
    modes\n    # For the RestFul API, we can't easily scale this up so MaxSize is
    largely ignored\n    export FLUX_REQUIRE_AUTH=true\n    export FLUX_SECRET_KEY=799c6de6-2789-4044-bd15-8e187bca744b\n
    \   export FLUX_NUMBER_NODES=4\n\n    printf \"\\n \U0001F511 Use your Flux user
    and token credentials to authenticate with the MiniCluster with flux-framework/flux-restful-api\\n\"\n\n
    \   # -o is an \"option\" for the broker\n    # -S corresponds to a shortened
    --setattr=ATTR=VAL \n    printf \"\\n\U0001F300 ${asFlux}  flux broker --config-path
    /etc/flux/config ${brokerOptions} ${startServer}\\n\"\n    ${asFlux}  flux broker
    --config-path /etc/flux/config ${brokerOptions} ${startServer}\n}\n\n# Run an
    interactive cluster, giving no command to flux start\nfunction run_interactive_cluster()
    {\n    printf \"\\n\U0001F300 ${asFlux}  flux broker --config-path /etc/flux/config
    ${brokerOptions}\\n\"\n    ${asFlux}  flux broker --config-path /etc/flux/config
    ${brokerOptions}\n}\n\n# Are we running diagnostics or the start command?\nif
    [ \"${diagnostics}\" == \"true\" ]; then\n    run_diagnostics\nelse\n\n    # Start
    flux with the original entrypoint\n    if [ $(hostname) == \"${mainHost}\" ];
    then\n\n        # If it's a batch job, we write the script for the broker to run\n
    \        # end if container batch\n\n        # Commands only run by the broker\n
    \        > /dev/null 2>&1\n\n        # No command - use default to start server\n\n
    \       if [ \"$@\" == \"\" ]; then\n\n            # An interactive job also doesn't
    require a command\n            run_flux_restful\n\n        # Case 2: Fall back
    to provided command\n        else\n\n            # If we are running a batch job,
    no launcher mode\n             # else for if container.batch\n            \n            \n
    \           ${asFlux} flux start  -o --config /etc/flux/config ${brokerOptions}
    \  flux submit  -n 2 --quiet  --watch $@\n             # end if container.launcher\n
    \            # end if container.batch\n        fi\n    else\n\n       # Commands
    only run by the workers\n        > /dev/null 2>&1\n\n        # Sleep until the
    broker is ready\n        \n        while true\n        do\n            ${asFlux}
    \ flux start -o --config /etc/flux/config ${brokerOptions}\n            retval=$?\n
    \           \n            if [[ \"${retval}\" -eq 0 ]]; then\n                \n
    \               break\n            fi\n            \n            sleep 15\n        done\n
    \   fi\n\n    \nfi\n\n"